<html><head>
    <meta content="text/html; charset=windows-1252" http-equiv="content-type">
  </head>
  <body><span style="font-family: Verdana;"> </span>
    <h1><span style="font-family: Verdana;"> MLCC - Laboratory 3 -
        Dimensionality reduction and feature selection </span></h1>
    <span style="font-family: Verdana;"> <br>
      In this laboratory we will address the problem of data analysis with a
      reference to a classification problem. <br>
      Follow the instructions below. Think hard before you call the instructors!<br>
      <br>
      Download:<br>
    </span>
    <ul>
      <li><span style="font-family: Verdana;"><a title="lab3 source code" href="./lab3.zip">zipfile</a>
          (unzip it in a local folder)</span></li>
    </ul>
    <span style="font-family: Verdana;"> </span>
    <h2 class="western" style="margin-top: 0in; margin-bottom: 0in"><font face="Verdana">1.
Warm up - data generation </font>
</h2>
<p style="margin-bottom: 0in"><br>
</p>
<p style="margin-bottom: 0in"><font face="Verdana">You will generate
a training and a test set of D-dimensional points (N points for each
class), with N=100 D=30. Only two of those dimensions will be meaningful, the other ones will be an irrelevant noise.</font></p>
<p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">N=100;</font></p>
<p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">D=30;</font></p>
<ul>
	<li><p style="margin-bottom: 0in"><strong><font face="Verdana">1.A</font></strong>
	<font face="Verdana">For each point, the first two variables will be
	generated by MixGauss, extracted from two gaussian distributions
	with centroids (1, 1) and (-1,-1)</font> <font face="Verdana">and
	standard deviation 0.7</font> <font face="Verdana">(the first one with Y=1, the
	second </font><font face="Verdana">with </font><font face="Verdana">Y=-1)</font>
		</p>
</li></ul>
<p style="margin-left: 0.42in; margin-bottom: 0in"><a name="__DdeLink__24_2095450872"></a>
<font face="Courier New, Courier, monospace">[Xtr, Ytr] =
MixGauss(...);</font></p>
<p style="margin-left: 0.42in; margin-bottom: 0in"><font face="Courier New, Courier, monospace">Ytr(Ytr==2)= -1;</font></p>
<p style="margin-left: 0.42in; margin-bottom: 0in"><font face="Courier New, Courier, monospace">[Xts,
Yts] = MixGauss(...); </font>
</p>
<p style="margin-left: 0.42in; margin-bottom: 0in"><font face="Courier New, Courier, monospace">Yts(Yts==2)
= -1;</font></p>
<ul>
	<li><p style="margin-bottom: 0in"><strong><font face="Verdana">1B.</font></strong>
	<font face="Verdana">You may want to plot the relevant variables of
	the data<br></font><font face="Courier New, Courier, monospace">scatter(Xtr(:,1),
	Xtr(:,2), 50, Ytr, 'filled');</font></p>

	<p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">hold on;</font></p>

    <p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">scatter(Xts(:,1),
	Xts(:,2), 50, Yts);</font></p>

    </li><li><p style="margin-bottom: 0in"><a name="__DdeLink__10_628866062"></a>
	<strong><font face="Verdana">1.C</font></strong><strong> </strong><font face="Verdana">The
	remaining variables will be generated as gaussian
	noise<br></font>
        <p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">sigma_noise = 0.01;<br>
	</p>
        <font face="Courier New, Courier, monospace">Xtr_noise=sigma_noise*randn(2*N,D-2);</font></p>
	<p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">Xts_noise=sigma_noise*randn(2*N,D-2);<br>
	</p>
	<p style="margin-bottom: 0in"><font face="Verdana">To compose the
	final data matrix, </font><font face="Verdana">run:</font></p>
	<p style="margin-bottom: 0in"><a name="__DdeLink__26_628866062"></a><font face="Courier New, Courier, monospace">Xtr
	=[Xtr, Xtr_noise];</font></p>
	<p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">Xts
	=[Xts, Xts_noise];</font></p>
</li></ul>
<h2 class="western"><font face="Verdana">2. Principal Component
Analysis</font></h2>
<ul>
	<li><p style="margin-bottom: 0in"><strong><font face="Verdana">2.A</font></strong><strong>
	</strong><font face="Verdana">Compute the data principal components
	(see </font><font face="Courier, monospace">help PCA</font><font face="Verdana">)</font></p>
	</li><li><p style="margin-bottom: 0in"><strong><font face="Verdana">2.B</font></strong>
	<font face="Verdana">Plot the first two components of X_proj</font>
	<font face="Verdana">using the following line</font></p>
	<p style="margin-bottom: 0in"><a name="__DdeLink__26_2095450872"></a>
	<font face="Courier, monospace">scatter(X_proj(:,1), X_proj(:,2),
	50, Ytr, 'filled');</font></p>
	</li><li><p style="margin-bottom: 0in"><font face="Verdana"><b>2.C </b></font><font face="Verdana">Try
	now with the first 3 components, by using</font></p>
	<p style="margin-bottom: 0in"><a name="__DdeLink__28_2095450872"></a><a name="__DdeLink__34_1406087933"></a>
	<font face="Courier, monospace">scatter3(X_proj(:,1), X_proj(:,2),
	X_proj(:,3), 50, Ytr, 'filled');</font></p>
	<p style="margin-bottom: 0in"><font face="Verdana">Reason on the
	meaning of the results you are obtaining</font>
	</p>
	</li><li><p style="margin-bottom: 0in"><a name="__DdeLink__30_2095450872"></a>
	<strong><font face="Verdana">2.D </font></strong><font face="Verdana">Display
	the sqrt of the first 10 eigenvalues (</font><font face="Courier, monospace">disp(sqrt(d</font><font face="Courier, monospace">(1:10)</font><font face="Courier, monospace">))</font><font face="Verdana">).
	Plot the coefficients (eigenvector) associated with the largest
	eigenvalue:</font></p>
	<p style="margin-bottom: 0in"><a name="__DdeLink__32_2095450872"></a><a name="__DdeLink__27_1019962586"></a>
	<font face="Courier, monospace">scatter(1:D, abs(V(:,1))) </font>
	</p>
	</li><li><p style="margin-bottom: 0in"><a name="__DdeLink__34_2095450872"></a>
	<font face="Verdana"><b>2.E</b></font> <font face="Verdana">Repeat
	the above steps</font> <font face="Verdana">with dataset generated
	using different sigma_noise (0, 0.01, </font><font face="Verdana, sans-serif">0.1,
	0.5, 0.7,</font> <font face="Verdana, sans-serif">1, 1.2, 1.4, 1.6,
	2). To what extent data visualization by PCA is affected by the
	noise?</font></p>
</li></ul>
<h2 class="western" style="margin-top: 0in; margin-bottom: 0in"><font face="Verdana">3.
Variable selection</font></h2>
<ul>
	<li><p style="margin-bottom: 0in"><a name="__DdeLink__36_2095450872"></a>
	<strong><font face="Verdana">3.A</font></strong> <font face="Verdana">Use
	the data generated in section 1. S</font><font face="Verdana">tandardize
	the data matrix, so that each column has mean 0 and standard
	deviation 1<br><br></font><font face="Courier New, Courier, monospace">m=mean(Xtr);
	(see "help mean", it computes the mean for each column)</font></p>
	<p style="margin-bottom: 0in"><font face="Courier New, Courier, monospace">s
	= std(Xtr);<br>for i = 1:2*N<br>    Xtr(i,:) = Xtr(i,:) - m;<br>end<br>for
	i = 1:2*N<br>	    Xtr(i,:) = Xtr(i,:) ./ s;<br>end</font></p>
	<p style="margin-bottom: 0in"><font face="Verdana, sans-serif">Do
	the same for Xts, by using m and s computed on Xtr.</font></p>
</li></ul>
<ul>
	<li><p style="margin-bottom: 0in"><strong><font face="Verdana">3.B</font></strong>
	<font face="Verdana">Use the orthogonal matching pursuit algorithm
	(type '</font><font face="Courier New, Courier, monospace">help
	OMatchingPursuit</font><font face="Verdana">')</font></p>
	</li><li><p style="margin-bottom: 0in"><a name="__DdeLink__29_1019962586"></a>
	<strong><font face="Verdana">3.C</font></strong> <font face="Verdana">You
	may want to check the predicted labels on the training set <br></font><font face="Courier New, Courier, monospace">Ypred</font>
	<font face="Courier New, Courier, monospace">= sign(Xts</font> <font face="Courier New, Courier, monospace">*
	w);<br>err = calcErr(Yts, Ypred);<br></font><font face="Verdana, sans-serif">and
	plot the coefficients </font><font face="Courier New, monospace">w</font><font face="Verdana, sans-serif">
	with </font><font face="Courier, monospace">scatter(1:D, abs(w))</font><font face="Verdana, sans-serif">.
	How the error changes with the number of iterations of the method? </font>
	</p>
	</li><li><p style="margin-bottom: 0in"><a name="__DdeLink__44_2095450872"></a><a name="__DdeLink__42_2095450872"></a>
	<font face="Verdana, sans-serif"><b>3.D</b></font> <font face="Verdana, sans-serif">By
	using the method </font><font face="Courier, monospace">holdoutCVOMP
	</font><font face="Verdana, sans-serif">find the best number of
	iterations </font><font face="Verdana, sans-serif">w</font><font face="Verdana, sans-serif">ith
	</font><font face="Courier, monospace">intIter = 2:D</font><font face="Verdana, sans-serif"> (and, for instance, perc=0.75 nrip = 20).</font></p>
	<p style="margin-bottom: 0in"><font face="Verdana, sans-serif">Moreover,
	plot the training and validation error with the following lines:</font></p>
	<p style="margin-bottom: 0in"><a name="__DdeLink__40_2095450872"></a>
	<font face="Courier, monospace">figure; </font>
	</p>
	<p style="margin-bottom: 0in"><font face="Courier, monospace">plot(intIter,
	Tm, 'r'); </font>
	</p>
	<p style="margin-bottom: 0in"><font face="Courier, monospace">hold
	on; </font>
	</p>
	<p style="margin-bottom: 0in"><font face="Courier, monospace">plot(intIter,
	Vm, 'b'); </font>
	</p>
	<p style="margin-bottom: 0in"><font face="Courier, monospace">hold
	off;</font></p>
	<p style="margin-bottom: 0in"><font face="Verdana, sans-serif">What
	is the behavior of the training and the validation errors with
	respect to the number of iterations?</font></p>

    </li><li><p style="margin-bottom: 0in"><strong><font face="Verdana">3.E </font></strong><font face="Verdana">Try
 to increase the number of relevant variables d = 3,5,.. (and the
corresponding standard deviation of the Gaussians) around the centroids
        </font></p><p><font face="Verdana"><font face="Courier, monospace">ones(d,1); % vector of all 1s</font></font></p><font face="Verdana"> and
        <p><font face="Courier, monospace">-ones(d,1); % vector of all -1s</font></p>
        and see how this change is reflected in the cross-validation.</font><p></p></li>
</ul>
<h2 class="western" style="margin-top: 0in; margin-bottom: 0in"><font face="Verdana">4.
If you have time - More experiments</font></h2>
<ul>
	<li><p style="margin-bottom: 0in"><strong><font face="Verdana">4.A</font></strong>
	<font face="Verdana">Analyse the results you obtain on sections 2
	and 3 once you choose</font>
	</p>
	<ul>
		<li><p style="margin-bottom: 0in"><font face="Verdana">N &gt;&gt; D</font>
				</p>
		</li><li><p style="margin-bottom: 0in"><font face="Verdana">N ~ D</font>
				</p>
		</li><li><p style="margin-bottom: 0in"><font face="Verdana">N &lt;&lt; D</font></p>
		<p style="margin-bottom: 0in"><font face="Verdana">and evaluate the
		benefits of the two different analysis</font></p>
	</li></ul>
</li></ul>

</body></html>
